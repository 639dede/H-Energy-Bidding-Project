import numpy as np
from scipy.stats import truncnorm
from sklearn.linear_model import LinearRegression

class TruncatedGMM:
    def __init__(self, n_components, c, d, max_iter=100, tol=1e-4):
        self.n_components = n_components
        self.c = c
        self.d = d
        self.max_iter = max_iter
        self.tol = tol
        # Parameters: for each component, we have coefficients for mu and sigma
        self.mu_models = [LinearRegression() for _ in range(n_components)]
        self.sigma_models = [LinearRegression() for _ in range(n_components)]
        self.pi = np.ones(n_components) / n_components  # Initialize equally

    def _initialize_parameters(self, a, b):
        # Initialize using KMeans or random selection
        from sklearn.cluster import KMeans
        kmeans = KMeans(n_clusters=self.n_components).fit(b.reshape(-1, 1))
        labels = kmeans.labels_
        for k in range(self.n_components):
            mask = labels == k
            if np.sum(mask) == 0:
                self.mu_models[k].fit(a, b)
                self.sigma_models[k].fit(a, b)
                continue
            self.mu_models[k].fit(a[mask], b[mask])
            residuals = b[mask] - self.mu_models[k].predict(a[mask])
            self.sigma_models[k].fit(a[mask], np.abs(residuals))
            self.pi[k] = np.mean(labels == k)

    def fit(self, a, b):
        self._initialize_parameters(a, b)
        log_likelihood_old = None

        for iteration in range(self.max_iter):
            # E-Step
            responsibilities = np.zeros((a.shape[0], self.n_components))
            for k in range(self.n_components):
                mu_k = self.mu_models[k].predict(a)
                sigma_k = np.exp(self.sigma_models[k].predict(a))  # Ensure positivity
                pdf = truncnorm.pdf(b.flatten(), 
                                    (self.c - mu_k) / sigma_k, 
                                    (self.d - mu_k) / sigma_k, 
                                    loc=mu_k, scale=sigma_k)
                responsibilities[:, k] = self.pi[k] * pdf
            # Normalize responsibilities
            sum_responsibilities = responsibilities.sum(axis=1, keepdims=True)
            responsibilities /= sum_responsibilities

            # M-Step
            for k in range(self.n_components):
                # Weighted regression for mu and sigma
                weights = responsibilities[:, k]
                # Update mu
                self.mu_models[k].fit(a, b.flatten(), sample_weight=weights)
                mu_k = self.mu_models[k].predict(a)
                # Update sigma
                residuals = b.flatten() - mu_k
                self.sigma_models[k].fit(a, np.abs(residuals), sample_weight=weights)
                # Update pi
                self.pi[k] = np.mean(weights)

            # Compute log-likelihood
            log_likelihood = np.sum(np.log(sum_responsibilities))
            if log_likelihood_old is not None and np.abs(log_likelihood - log_likelihood_old) < self.tol:
                print(f'Converged at iteration {iteration}')
                break
            log_likelihood_old = log_likelihood
            print(f'Iteration {iteration}, Log-Likelihood: {log_likelihood}')

    def predict_proba(self, a, b):
        responsibilities = np.zeros((a.shape[0], self.n_components))
        for k in range(self.n_components):
            mu_k = self.mu_models[k].predict(a)
            sigma_k = np.exp(self.sigma_models[k].predict(a))
            pdf = truncnorm.pdf(b.flatten(), 
                                (self.c - mu_k) / sigma_k, 
                                (self.d - mu_k) / sigma_k, 
                                loc=mu_k, scale=sigma_k)
            responsibilities[:, k] = self.pi[k] * pdf
        sum_responsibilities = responsibilities.sum(axis=1, keepdims=True)
        return responsibilities / sum_responsibilities
    
    
# Example data
a = np.random.rand(10000, 1)
b = 2 * a + np.random.randn(10000, 1) * 0.5
# Apply truncation bounds
c, d = 0.5, 1.5
mask = (b >= c) & (b <= d)
a_filtered = a[mask].reshape(-1, 1)
b_filtered = b[mask].reshape(-1, 1)

# Initialize and fit the model
n_components = 3  # Choose based on your data

gmm = TruncatedGMM(n_components=n_components, c=c, d=d)
gmm.fit(a_filtered, b_filtered)

# Predict probabilities for new data
# a_new = np.array([...]).reshape(-1, 1)
# b_new = np.array([...]).reshape(-1, 1)
# probs = gmm.predict_proba(a_new, b_new)

print(a, b)